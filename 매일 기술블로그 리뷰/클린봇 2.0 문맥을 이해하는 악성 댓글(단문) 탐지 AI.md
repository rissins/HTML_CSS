# 매일 기술블로그 Review

# 2021-10-25

### / 링크

[NAVER D2](https://d2.naver.com/helloworld/7753273)

### / 소감

// 인터넷이 출시함으로써 셀 수 없는 많은 장점들을 우리에게 안겨줬다. 하지만 장점이 있으면 그에 반대되는 단점도 당연히 생기기 마련이다. 그 중 가장 문제가 되는건 악성 댓글(악플)로 생각된다. 악플로 인해 많은 유명인들이 안타까운 선택을 하고 옳지않은 방법을 택하기도 하기때문에 사이버상에서 해결해 나가야 할 큰 문제점이다. 네이버는 악플을 줄이기 위해 '클린봇 2.0 '이름을 가진 탐지 AI를 발표하였는데 그 과정에 흥미가 생겨 읽게되었다. 순서로는 크게 데이터셋 구축 → 모델링 → 학습 → 전이 학습 → 최종모델 선택 순서로 진행된다.

 간단히 악플의 판단을 문자로 아닌 0~2사이의 값으로 분석을 한다. 약 35만개의 댓글을 분석해 데이터셋을 구축하여 지정해둔 요구 사항을 만족하는 댓글은 악플 여부를 판단해주는 척도가 된다.  그 기준으로 특정 임계 값을 벗어나면 악플로 판단한 뒤 적절한 조치를 취한다. 댓글에 대한 결과를 얻기 위해서는 단어, 형태소, 음절 등의 토큰 단위로 토큰화(tokenizing)을 해야한다. 

 모델 구조 설계에는 BERT와 같은 무거운 SOTA 모델을 사용하기보다는 비교적 가벼운 수준의 규모와 연산 복잡도를 유지하는 선에서 뉴럴 넷구조를 설계하려 했다고 한다. 그 이유에는 4가지가 있다.

1. 정의된 태스크가 단순하다.
2. 모델 파라미터의 증가보다는 데이터의 충원에 의한 성능 향상 효과가 훨씬 뚜렷했다.
3. 고정된 데이터에서 최고의 성능을 끌어올리는 것보다 빠른 댓글 트렌드 변화에 따라 지속적으로 업데이트를 하는 것이 악플 문제에 더 효과적이다.
4. 댓글 작성에 대한 사용자 경험이 저해되지 않기 위해서 최소의 응답 속도와 서비스 안정성을 보장해야 한다.

 그 다음 오버샘플링로 데이터의 균형을 맞춰준 뒤 AI가 완벽하게 분석하는건 불가능하니 노이즈를 제거하기 위해 모델을 활용한 노이저 정제 방법을 사용해  잘못 레이블링한 사례를 최소화 해준다.  그 결과로 각 모델의 평가지표는 정제 전보다 더 상승한 값을 도출해낸다. 그리고 목표로 하는 태스크에 대해 모델을 학습하기 전에 관련성 있는 다른 태스크를 학습하는 과정을 먼저 거치게 함으로써 모델의 성능 향상을 꾀하는 방법인 전이 학습(Transfer Learning)을 실행한다. 

 전이 학습을 통해 최종적으로 도출된 결과로 가장 이상적인 값을 내주는 모델을 최종 선택한다.  이런 과정을 통해서 나온 클린봇 2.0이 우리가 흔히 네이버 뉴스의 댓글에 '클린봇에 의해서 삭제된 댓글입니다.' 를 해주는 AI가 탄생하게 된 것이다.

 이 포스팅은 제목에 이끌려서 자연스레 읽게 되었다. 한 번도 겪어보지못한 알고리즘 AI부분이라 낯선 단어와 수식들이 나를 반겼다. 깊게는 아니지만 'SOTA', 'BERT'와 같은 용어들을 검색 해봄으로써 생각의 폭이 조금이나마 더 넓어진 것 같다.

-Reference

[https://m.blog.naver.com/dnjswns2280/222028982187](https://m.blog.naver.com/dnjswns2280/222028982187)

[https://ebbnflow.tistory.com/151](https://ebbnflow.tistory.com/151)